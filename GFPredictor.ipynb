{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Conv2d, Linear\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import MaxPool2d, AvgPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn.modules.batchnorm import BatchNorm1d, BatchNorm2d\n",
    "from torch.nn import LogSoftmax, Sigmoid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageSequence\n",
    "from torchvision.io import read_image\n",
    "from torch import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new folder for png images\n",
    "if not os.path.exists('png'):\n",
    "    os.makedirs('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 2 #write here the number of the GFP channel\n",
    "measurementFileName='measurements3.csv' #write here the name of the measurement file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = pd.DataFrame()\n",
    "#iterating over the images\n",
    "for i in range(2, 3):\n",
    "    for j in range(1,4):\n",
    "        try:\n",
    "            #loading the csv file of the corresponding image\n",
    "            data = pd.read_csv(f'cells/Image_{i:03}_0{j}-centroids.csv', header=None)\n",
    "            centroids = pd.concat([centroids, data], ignore_index=True)\n",
    "            #converting each image to png\n",
    "            for k in range(0, len(data)):\n",
    "                with Image.open('cells/' + data.iloc[k, 4] + '.tif') as im:\n",
    "                    tmp = np.asarray(ImageSequence.all_frames(im)[channel-1], dtype=np.uint8)[:64, :64]\n",
    "                    im2 = Image.fromarray(tmp)\n",
    "                    im2.save('png/'+ data.iloc[k, 4] + '.png')\n",
    "            print(f'Image_{i:03}_0{j} done')\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f'Image_{i:03}_0{j} not found')\n",
    "        except FileNotFoundError:\n",
    "            print(f'Image_{i:03}_0{j} not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = annotations_file\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 4]+'.png')\n",
    "        image = read_image(img_path)\n",
    "        \n",
    "        image = image.to(torch.float32)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, numChannels, hidensize, classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.relu = ReLU(inplace=True)\n",
    "        self.norm0 = BatchNorm2d(1)\n",
    "\n",
    "        # Layer 1\n",
    "        self.conv1 = Conv2d(in_channels=numChannels, out_channels=30, kernel_size=(5, 5))\n",
    "        self.norm1 = BatchNorm2d(30)\n",
    "        self.pool1avg = AvgPool2d(kernel_size=4, stride=1)\n",
    "        self.pool1max = MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        # Layer 2\n",
    "        self.conv2 = Conv2d(in_channels=30, out_channels=40, kernel_size=(5, 5))\n",
    "        self.norm2 = BatchNorm2d(40)\n",
    "        self.pool2max = MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 3\n",
    "        self.conv3 = Conv2d(in_channels=40, out_channels=50, kernel_size=(5, 5))\n",
    "        self.norm3 = BatchNorm2d(50)\n",
    "        self.pool3max = MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 4\n",
    "        self.conv4 = Conv2d(in_channels=50, out_channels=60, kernel_size=(5, 5))\n",
    "        self.norm4 = BatchNorm2d(60)\n",
    "        self.pool4max = MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Classification\n",
    "        self.fc1 = Linear(in_features=540, out_features=hidensize)\n",
    "        self.fc2 = Linear(in_features=hidensize, out_features=hidensize)\n",
    "        self.fc3 = Linear(in_features=hidensize, out_features=hidensize)\n",
    "        self.fc4 = Linear(in_features=hidensize, out_features=classes)\n",
    "        self.activation = LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.norm0(x)\n",
    "        \n",
    "        # First Layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1avg(x)\n",
    "        x = self.pool1max(x)\n",
    "        # Second Layer\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2max(x)\n",
    "        # Third Layer\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool3max(x)\n",
    "        # Fourth Layer\n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool4max(x)\n",
    "        # Linear\n",
    "        x = flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x) # Dropouts avoid overfit\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.dropout(x)\n",
    "        # Classifier\n",
    "        output = self.activation(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model\n",
    "modl = LeNet(1, 500, 2)\n",
    "path = 'model.zip'\n",
    "if not torch.cuda.is_available():\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')\n",
    "modl.load_state_dict(torch.load(path))\n",
    "\n",
    "#creating the dataset with the images you want to classify\n",
    "test = CustomImageDataset(centroids, './png')\n",
    "testldr = DataLoader(dataset=test, batch_size=20, shuffle=False)\n",
    "modl.eval()\n",
    "\n",
    "#predicting the labels\n",
    "labels = np.array([])\n",
    "for img in testldr:\n",
    "    with torch.no_grad():\n",
    "        testoutp = modl(img)\n",
    "        pred = torch.max(testoutp, 1)[1].data.squeeze()\n",
    "        labels = np.append(labels, pred.numpy())\n",
    "\n",
    "#adding the labels to the dataframe\n",
    "centroids.iloc[:,1]=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the predicted labels with the measurements\n",
    "measures = pd.read_csv(measurementFileName, sep=',')\n",
    "centroids.iloc[:,2] = [round(i, 1) for i in centroids.iloc[:,2]]\n",
    "centroids.iloc[:,3] = [round(i, 1) for i in centroids.iloc[:,3]]\n",
    "new_meas = measures.merge(centroids, left_on=['Image', 'Centroid X µm', 'Centroid Y µm'], right_on=[0, 2, 3], how='left').drop(columns=[0, 2, 3, 4]).rename(columns={1: 'Prediction'})\n",
    "print(f'{sum(new_meas[\"Prediction\"].isna())-sum(measures[\"Nucleus: Area µm^2\"]<50)} neurons could not find a prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the measurements with the predicted labels!\n",
    "new_meas.to_csv('measurements4.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6ff92366812eff0585597bdff8203f3dca47fc152c7f8518e1cf16bc394b74b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
